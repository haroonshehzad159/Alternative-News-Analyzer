{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3092c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'punkt' to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'punkt_tab' to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'stopwords' to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download attempts finished.\n",
      "Successfully imported word_tokenize.\n",
      "word_tokenize test successful: ['This', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# import os\n",
    "\n",
    "# custom_nltk_data_path = \"C:\\\\nltk_data\"\n",
    "# if not os.path.exists(custom_nltk_data_path):\n",
    "#     try:\n",
    "#         os.makedirs(custom_nltk_data_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not create folder {custom_nltk_data_path}: {e}\")\n",
    "\n",
    "# if custom_nltk_data_path not in nltk.data.path:\n",
    "#     nltk.data.path.append(custom_nltk_data_path)\n",
    "\n",
    "# print(f\"Attempting to download 'punkt' to {custom_nltk_data_path}...\")\n",
    "# nltk.download('punkt', download_dir=custom_nltk_data_path, quiet=False, force=True)\n",
    "\n",
    "\n",
    "# print(f\"Attempting to download 'punkt_tab' to {custom_nltk_data_path}...\")\n",
    "# nltk.download('punkt_tab', download_dir=custom_nltk_data_path, quiet=False, force=True)\n",
    "\n",
    "\n",
    "# print(f\"Attempting to download 'stopwords' to {custom_nltk_data_path}...\")\n",
    "# nltk.download('stopwords', download_dir=custom_nltk_data_path, quiet=False, force=True)\n",
    "\n",
    "# print(\"Download attempts finished.\")\n",
    "\n",
    "# # Quick test\n",
    "# try:\n",
    "#     from nltk.tokenize import word_tokenize\n",
    "#     print(\"Successfully imported word_tokenize.\")\n",
    "#     test_tokens = word_tokenize(\"This is a test sentence for tokenization.\")\n",
    "#     print(\"word_tokenize test successful:\", test_tokens)\n",
    "# except Exception as e:\n",
    "#     print(\"Error during quick test of word_tokenize:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce68164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\nltk_data is already in NLTK data path.\n",
      "NLTK stopwords found.\n",
      "NLTK punkt tokenizer found.\n",
      "--- Testing Article Fetching and NLTK Preprocessing ---\n",
      "Attempting to fetch article from: https://www.bbc.com/news/articles/c87j5v4xjxqo\n",
      "Successfully fetched: Sidhu Moose Wala: Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "Title: Sidhu Moose Wala: Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "--- Raw Content (first 500 chars) ---\n",
      "Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "11 June 2025 Share Save Soutik Biswas & Ishleen Kaur BBC Eye Investigations Share Save\n",
      "\n",
      "BBC Sidhu Moose Wala was shot dead in a hail of bullets in 2022\n",
      "\n",
      "It was a killing that shocked India: Punjabi hip-hop star Sidhu Moose Wala shot dead through the windscreen of his car by hired gunmen. Within hours, a Punjabi gangster named Goldy Brar had used Facebook to claim responsibility for ordering the hit. But three years after the murde...\n",
      "\n",
      "--- NLTK Processed Content (first 1000 chars) ---\n",
      "gangster tells bbc india biggest hip hop star murdered june share save soutik biswas ishleen kaur bbc eye investigations share save bbc sidhu moose wala shot dead hail bullets killing shocked india punjabi hip hop star sidhu moose wala shot dead windscreen car hired gunmen within hours punjabi gangster named goldy brar used facebook claim responsibility ordering hit three years murder one faced trial goldy brar still run whereabouts unknown bbc eye managed make contact brar challenged sidhu moose wala became target response coldly articulate arrogance moose wala made mistakes could forgiven brar told bbc world service option kill face consequences actions either simple watch gunmen staking moose wala house warm may evening sidhu moose wala taking black mahindra thar suv usual spin dusty lanes near village northern indian state punjab within minutes two cars began tailing cctv footage later showed weaving narrow turns sticking close bend road one vehicles lurched forward cornering moose...\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and Article Fetching/Preprocessing Setup\n",
    "\n",
    "import nltk\n",
    "import os \n",
    "from newspaper import Article \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- Point NLTK to a specific data directory ---\n",
    "\n",
    "custom_nltk_data_path = \"C:\\\\nltk_data\" \n",
    "\n",
    "if not os.path.exists(custom_nltk_data_path):\n",
    "    try:\n",
    "        os.makedirs(custom_nltk_data_path)\n",
    "        print(f\"Created NLTK data directory: {custom_nltk_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create NLTK data directory {custom_nltk_data_path}. Please create it manually. Error: {e}\")\n",
    "\n",
    "if custom_nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(custom_nltk_data_path)\n",
    "    print(f\"Added {custom_nltk_data_path} to NLTK data path.\")\n",
    "else:\n",
    "    print(f\"{custom_nltk_data_path} is already in NLTK data path.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check if resources are found in ANY of the NLTK paths\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"NLTK stopwords found.\")\n",
    "except LookupError:\n",
    "    print(f\"Downloading NLTK stopwords to {custom_nltk_data_path}...\")\n",
    "    nltk.download('stopwords', download_dir=custom_nltk_data_path, quiet=False) # quiet=False for visibility\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK punkt tokenizer found.\")\n",
    "except LookupError:\n",
    "    print(f\"Downloading NLTK punkt tokenizer to {custom_nltk_data_path}...\")\n",
    "    nltk.download('punkt', download_dir=custom_nltk_data_path, quiet=False) # quiet=False for visibility\n",
    "\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "def fetch_and_preprocess_article_nltk(url):\n",
    "    \"\"\"Fetches article from URL and preprocesses its text using NLTK.\"\"\"\n",
    "    print(f\"Attempting to fetch article from: {url}\")\n",
    "    try:\n",
    "        article_obj = Article(url)\n",
    "        article_obj.download()\n",
    "        article_obj.parse()\n",
    "        raw_text = article_obj.text\n",
    "        title = article_obj.title\n",
    "        # Check if title is None or empty, which can happen if parsing fails\n",
    "        if not title:\n",
    "            title = \"Title not found\"\n",
    "            print(\"Warning: Article title not found by newspaper3k.\")\n",
    "        else:\n",
    "            print(f\"Successfully fetched: {title}\")\n",
    "\n",
    "        \n",
    "        if not raw_text or len(raw_text) < 100: \n",
    "             print(f\"Warning: Fetched raw text is very short or None for {url}. Content might be missing or paywalled/JS-rendered.\")\n",
    "             \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching/parsing article from {url}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if not raw_text: \n",
    "        print(f\"No content found for article (raw_text is None): {url}\")\n",
    "        return None, title, None \n",
    "\n",
    "    # NLTK Preprocessing\n",
    "    text_lower = raw_text.lower()\n",
    "    text_no_punct = re.sub(r'\\W', ' ', text_lower) \n",
    "    text_no_extra_space = re.sub(r'\\s+', ' ', text_no_punct).strip() \n",
    "    \n",
    "    tokens = word_tokenize(text_no_extra_space) \n",
    "    \n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word not in stop_words_nltk and len(word) > 2 and word.isalpha() \n",
    "    ]\n",
    "    preprocessed_text = \" \".join(filtered_tokens)\n",
    "\n",
    "    return raw_text, title, preprocessed_text\n",
    "\n",
    "# --- Test the function ---\n",
    "test_article_url = 'https://www.bbc.com/news/articles/c87j5v4xjxqo'\n",
    "\n",
    "print(\"--- Testing Article Fetching and NLTK Preprocessing ---\")\n",
    "raw_content, article_title, processed_content_nltk = fetch_and_preprocess_article_nltk(test_article_url)\n",
    "\n",
    "if raw_content:\n",
    "    print(f\"\\nTitle: {article_title}\") \n",
    "    print(\"\\n--- Raw Content (first 500 chars) ---\")\n",
    "    print(raw_content[:500] + \"...\")\n",
    "    if processed_content_nltk:\n",
    "        print(\"\\n--- NLTK Processed Content (first 1000 chars) ---\")\n",
    "        print(processed_content_nltk[:1000] + \"...\")\n",
    "    else:\n",
    "        print(\"\\n--- NLTK Processed Content: Not generated (likely due to issues with raw_content or tokenization).\")\n",
    "\n",
    "\n",
    "    documents_for_bertopic = [raw_content] if raw_content else []\n",
    "    processed_documents_for_bertopic = [processed_content_nltk] if processed_content_nltk else []\n",
    "else:\n",
    "    print(f\"\\nCould not get raw content for article: {test_article_url}\")\n",
    "    \n",
    "    documents_for_bertopic = []\n",
    "    processed_documents_for_bertopic = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92bd13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
